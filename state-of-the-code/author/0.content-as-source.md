# ------------------- Author's Original Version ---------------------

We do need a database. At first, the initial approach to article authorship was database-driven: articles were written in YAML (and previously Markdown) and stored in a database. A wildcard slug route loaded each article dynamically from serialized database data.

This new version moves away from that, and I intend to keep moving in that direction. The latest multimodal components reflect this shift. My current hypothesis is that it’s a good idea to treat certain things differently: I want articles to be fully interactive when needed, which requires React. I also want type completion, so I want to author articles directly in source code. At the same time, I need to access articles programmatically for search, embedding generation, and retrieval-augmented generation (RAG).

The multimodal components enable this:
	•	They can render articles as server-side components with client-side islands.
	•	They can also render into Markdown, either for a full article or for specific sections.

This is useful because we can generate embeddings at the section level for RAG, search, and the chat assistant. It also supports OpenGraph metadata, and other derived outputs. Metadata can be embedded in components and selectively rendered or hidden depending on the modality: rendering for a human reader, a crawler, or a chatbot may require different outputs.

Not all code should be multimodal. I think it makes sense to create a dedicated content/ directory:
	•	content/articles/: multimodal article components.
	•	content/contributors/: structured profiles describing contributors, their authored articles, and bios.
	•	content/terminology/: branded concepts, philosophy, and definitions that form our “gospel.”
	•	content/labs/: catalog entries describing labs, linking to them, and providing metadata for navigation, OpenGraph, and SEO.

Everything in content/ should be multimodal, because this is the canonical knowledge base we present—through the website, the chat assistant, or external crawlers.

By contrast, not all material is content under this definition:
	•	Community posts and comments: These must live in a database because they are user-generated and cannot be authored in advance. If the chat assistant ever needs to reference them, the RAG pipeline must query the database.
	•	App pages and routes: These define presentation and navigation (layouts, slugs, sidebars). They consume content, but they are not content themselves.
	•	Multimodal primitives and UI libraries: These belong in lib/, not content/. They are tools and building blocks, not knowledge artifacts.

So the structure looks like this:
	•	content/ = canonical, authored, multimodal knowledge.
	•	app/ = platform-level routes, navigation, presentation.
	•	lib/ = multimodal primitives and reusable UI tooling.

The dependency flow is one-directional:
	•	app consumes content and lib.
	•	content consumes lib.
	•	Neither lib nor content depend on app.

This approach is the opposite of a CMS. Instead of storing content in a database, we adopt content as source: the repository itself is the canonical store. The database remains essential for dynamic, user-generated material and for derived artifacts such as embeddings and caches, but our authored knowledge lives in source.

# ------------------- ChatGPT's Version ---------------------

Organized Expansion of Your Notes

Historical Context: Database-Centric vs. Source-Centric

Your initial architecture leaned on a database as the canonical store. Articles began as YAML/Markdown, serialized into a database, then dynamically rendered through a wildcard slug route. This enabled flexibility but locked authorship into a CMS-like model.

The new multimodal approach deliberately shifts away from that model: content is authored in source code (React/TypeScript files) using multimodal components, while the database plays a more supportive role (e.g., caching, indexing, storing derived embeddings). This inversion is what you describe as content as source—a philosophical reversal of the CMS paradigm.

Multimodal Authoring and Rendering

Multimodal components provide a dual mandate:
	1.	Interactive Article Surfaces
Articles authored as React components gain type completion, composability, and full React interactivity (client-side islands, dynamic sections).
	2.	Programmatic Derivations
The same content can be re-rendered into alternate modalities:
	•	Section → Markdown for embedding into a vector store (RAG context).
	•	Article → Markdown for OpenGraph/social previews.
	•	Article → Plain text for SEO indexers.
	•	Article → React tree for browser rendering.

The “rendering for a modality” principle makes explicit that metadata and component state can be included or excluded depending on whether the target is a chatbot, crawler, or human reader.

Canonical Directory Structure

You propose a clear split:
	•	content/
Houses all multimodal knowledge artifacts (articles, contributors, terminology, lab catalogs). Canonical, authored in source.
	•	content/articles/: multimodal articles.
	•	content/contributors/: profiles written as multimodal components.
	•	content/terminology/: branded concepts and philosophy (“the gospel”).
	•	content/labs/: catalog entries (metadata, descriptions, links) for lab features.
	•	app/
Houses routing and presentation (page layouts, slugs, navigation). These are not content; they consume and display content. The app is the platform layer: routing, indexing, and rendering infrastructure.
	•	lib/
Houses agnostic building blocks:
	•	Multimodal primitives (*.mm.srv.tsx) that define compositional semantics.
	•	UI primitives (non-multimodal, reusable presentational widgets).
These are tools, not knowledge.

This creates a directional dependency:
app consumes content and lib.
content consumes lib.
Neither lib nor content depend on app.

Distinction: What Counts as Content

You’re making an important categorical separation:
	•	Content (canonical, in repo):
Authored and owned knowledge. Articles, contributor bios, branded terminology, catalogs of labs.
	•	Not Content (dynamic, database-backed):
User-generated material such as posts, comments, article discussions. These cannot live in repo because they are not authored in advance. Instead they live in a database and may optionally be pulled into embeddings or RAG queries later.
	•	Not Content (platform, app layer):
Routing, layouts, sidebars, navigation. These are platform-level affordances, not semantic knowledge.

This separation ensures clarity of authorship: only material we control and curate lives in content/. Community material lives in the database. Platform affordances live in app/.

Implications for RAG and Search
	•	Canonical embeddings come from content/. Each section can yield a stable embedding, suitable for indexing and retrieval.
	•	Augmented embeddings may later include database material (comments, posts) but that is additive, not canonical.
	•	Search unification becomes natural: all content/ artifacts are indexable; dynamic data has its own derivation pipeline.

Conceptual Summary

Your working hypothesis is coherent:
	•	Replace the database-as-source with source-as-source (content-as-source).
	•	Treat multimodal components as the canonical representation of knowledge.
	•	Enforce a strict boundary: content is authored knowledge; app is platform; lib is tooling.
	•	Reserve the database for dynamic, user-generated material and derived artifacts (embeddings, caches).

This creates a two-pole system:
	•	Pole A: Canonical authored multimodal content in content/.
	•	Pole B: Dynamic user contributions and runtime data in the database.
The app/platform orchestrates both, but the canonical pole is always source-first.